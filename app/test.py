import os
import sys
import requests 
# Add the project root directory to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.logging_config import setup_logging
import logging
from modules.pdf_parser import extract_text_from_pdf, chunk_text
from modules.database_handler import DatabaseHandler
from app.config import CHUNK_SIZE, PDF_DIR, DATABASE_OPTION
import json

# Ollama API setup
OLLAMA_BASE_URL = "http://127.0.0.1:11434" 
MODEL_NAME="llama3.1"
def ask_model(prompt: str, model_name: str, endpoint_url: str) -> str:
    """
    Sends a prompt to the specified model server and retrieves the generated response.
    """
    url = f"{endpoint_url}/v1/completions"
    payload = {
        "prompt": prompt,
        "model": model_name,
        "temperature": 0.2,
        "top_p": 0.9,
        "top_k": 25,
        "max_tokens": 500  # Limit the response length
    }

    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()["choices"][0]["text"]
    except requests.exceptions.RequestException as e:
        return f"Error communicating with {model_name}: {e}"

def extract_final_answers(results):
    """
    Extracts and formats the final answers from query results.

    :param results: Query results returned by ChromaDB.
    :return: List of relevant answers.
    """
    final_answers = []
    if results and "documents" in results:
        for idx, document in enumerate(results["documents"][0]):
            metadata = results["metadatas"][0][idx] if "metadatas" in results else {}
            distance = results["distances"][0][idx] if "distances" in results else None

            answer = {
                "text": document.strip(),
                "chunk_id": metadata.get("chunk_id", "Unknown"),
                "distance": distance,
            }
            final_answers.append(answer)

    return final_answers



def generate_final_answer(filtered_chunks, user_query):
    """
    Generates the final answer using the filtered chunks and user query.

    :param filtered_chunks: List of filtered chunks to use as context.
    :param user_query: The user's query.
    :return: The final answer generated by the model.
    """
    # context = "\n".join([chunk["text"] for chunk in filtered_chunks])
    first_chunk = filtered_chunks[0]
    print("Full context:", filtered_chunks)
    print("\n \n Context received \n:", first_chunk)
    # prompt = f"I am providing you some context and a question. First answer my question based stricttly on the context Context:\n{first_chunk}\n\nQuestion: {user_query}\nAnswer:"
    # prompt = f"You are a helpful assistant. I am providing you some context and a question. First answer my question based stricttly on the context. Then, in the optional comments section share your insights and interpretations, if any. If there is a contradiction within the document, mention it in optional comments. If Context: {first_chunk['text']}\n\nQuestion: {user_query}\nAnswer: [your answer here according to context] \n Optional Comments: [your optional comments here, if any] \n"
    query_text = (
    "You are an assistant who answers questions strictly based on the provided Document Text. "
    "Your primary response should be based on the information given in the Document Text. "
    "Ensure that the final inference result is included in the <answer> tag. When making the inference, treat all information in the document as true.\n\n"
    f"Document Text:\n{first_chunk}\n\n"
    f"Question: {user_query}\n\n"
    "If the answer is not found in the Document Text, respond with 'No information available.' Only say 'No information available' "
    "if there is genuinely no relevant information in the Document Text.\n\n"
    "Please respond using the following XML format and provide the answer in key terms only (e.g., 'New Delhi' instead of 'The capital of India is New Delhi'):\n\n"
    "\n<response>\n"
    "    <answer>\n"
    "        [Your concise answer in key terms based strictly on the Document Text, or 'No information available']\n"
    "    </answer>\n\n"
    "    <comments>\n"
    "        [Any factual corrections, additional context, or your interpretation, if applicable]\n"
    "    </comments>\n"
    "</response>"
)
    
    
    return ask_model(query_text, MODEL_NAME, OLLAMA_BASE_URL)



def main():
    setup_logging()
    print("Starting the main function")
    
    # Initialize database handler
    db_handler = DatabaseHandler(option=DATABASE_OPTION)
    print("DatabaseHandler initialized")

    # Parse and process PDFs

    for pdf_file in os.listdir(PDF_DIR):
        if pdf_file.endswith(".pdf") and pdf_file == "sample_document.pdf":
            file_path = os.path.join(PDF_DIR, pdf_file)
            text = extract_text_from_pdf(file_path)
            print(f"Extracted text from {pdf_file}\n \n \n")
            print(f"Extracted text from {pdf_file}")
            chunks = chunk_text(text, CHUNK_SIZE)
            print(f"Chunked text into {len(chunks)} chunks")

            # Add chunks to the database
            # Clear the database
            # db_handler.collection.delete_many({})

            db_handler.add_documents(chunks)
            print(f"Added chunks to the database for {pdf_file}")

    # Test query
    # user_query = "Who is the daughter of Sandeep Raheja?"
    # user_query = " How far can the fence be located from its theoretical position?"
    user_query = "What is the capital of UAE?"
    # user_query = "What is the capital of India?"
    results = db_handler.query(user_query)
    logging.info(f"Query results: {results}")

    # Extract final answers
    final_answers = extract_final_answers(results)
    logging.info(f"Final answers: {final_answers}")

    # Generate final answer using the model
    final_answer = generate_final_answer(final_answers, user_query)
    logging.info(f"Final answer: {final_answer}")
    print("Final answer:", final_answer)

            





if __name__ == "__main__":
    main()
